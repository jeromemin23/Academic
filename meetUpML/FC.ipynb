{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from pytoune.framework import Model, ModelCheckpoint, CSVLogger\n",
    "from pytoune import torch_to_numpy\n",
    "from pytoune.layers import Flatten\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se déclare ici un réseau de neurones très simple:\n",
    "- Simple séparateur linéaire\n",
    "- 10 neurones\n",
    "- 784 poids par neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fully_connected = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Image shape is (1, 28, 28)\n",
    "        # View turns it into (1, 784)\n",
    "        image = image.view(-1, 28 * 28)\n",
    "        output = self.fully_connected(image)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fully_connected): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0109,  0.0271,  0.0339,  ..., -0.0201, -0.0120,  0.0302],\n",
       "         [ 0.0037,  0.0299, -0.0195,  ...,  0.0096,  0.0201, -0.0311],\n",
       "         [-0.0100,  0.0177, -0.0264,  ...,  0.0326, -0.0025,  0.0238],\n",
       "         ...,\n",
       "         [ 0.0333, -0.0176, -0.0191,  ..., -0.0095,  0.0280, -0.0354],\n",
       "         [-0.0252, -0.0156,  0.0130,  ..., -0.0174,  0.0173, -0.0241],\n",
       "         [ 0.0145,  0.0032,  0.0354,  ..., -0.0149, -0.0073,  0.0317]],\n",
       "        requires_grad=True), torch.Size([10, 784]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([-0.0270,  0.0183,  0.0291,  0.0096, -0.0270,  0.0218, -0.0220, -0.0298,\n",
       "          0.0273,  0.0265], requires_grad=True), torch.Size([10]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "train_split_percent = 0.8\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "n_epoch = 5\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST('./mnist/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "num_data = len(train_dataset)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = math.floor(train_split_percent * num_data)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "valid_indices = indices[split:]\n",
    "valid_dataset = Subset(valid_dataset, valid_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, pytorch_module):\n",
    "    # One nice feature of Pytoune is callbacks.\n",
    "    callbacks = []\n",
    "    \n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Pytoune Model\n",
    "    model = Model(pytorch_module, optimizer, loss_function, metrics=['accuracy'])\n",
    "\n",
    "    # Send model on GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Train\n",
    "    model.fit_generator(train_loader, valid_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "    # Test\n",
    "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 16.06s Step 1500/1500: loss: 0.838002, acc: 81.970833, val_loss: 0.528821, val_acc: 87.783333\n",
      "Epoch 2/5 16.52s Step 1500/1500: loss: 0.485561, acc: 87.683333, val_loss: 0.438331, val_acc: 89.125000\n",
      "Epoch 3/5 15.35s Step 1500/1500: loss: 0.426392, acc: 88.756250, val_loss: 0.401422, val_acc: 89.883333\n",
      "Epoch 4/5 13.35s Step 1500/1500: loss: 0.397059, acc: 89.291667, val_loss: 0.381339, val_acc: 90.275000\n",
      "Epoch 5/5 15.28s Step 1500/1500: loss: 0.378874, acc: 89.725000, val_loss: 0.367139, val_acc: 90.558333\n",
      "Test:\n",
      "\tLoss: 0.35133745963573454\n",
      "\tAccuracy: 90.65\n"
     ]
    }
   ],
   "source": [
    "train('fc', net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0101,  0.0251,  0.0314,  ..., -0.0187, -0.0111,  0.0280],\n",
       "        [ 0.0034,  0.0277, -0.0181,  ...,  0.0089,  0.0187, -0.0289],\n",
       "        [-0.0092,  0.0164, -0.0245,  ...,  0.0302, -0.0023,  0.0221],\n",
       "        ...,\n",
       "        [ 0.0309, -0.0164, -0.0177,  ..., -0.0088,  0.0260, -0.0329],\n",
       "        [-0.0234, -0.0144,  0.0120,  ..., -0.0162,  0.0161, -0.0223],\n",
       "        [ 0.0135,  0.0030,  0.0329,  ..., -0.0138, -0.0068,  0.0294]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0ba509908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(w[3].view(28, 28).data, cmap='gray_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
